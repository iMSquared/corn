_target_: pkm.models.rl.net.pw_attn.PatchWiseAttentionFeatNet.Config
dim_in: [8, 256]
dim_out: 256
keys: []
num_query: 4
ctx_dim: 48
emb_dim: 256
encoder:
  _target_: pkm.models.cloud.point_mae.PointMAEEncoder.Config
  layer:
    _target_: pkm.models.cloud.point_mae.PointMAELayer.Config
    attention:
      _target_: pkm.models.cloud.point_mae.PointMAEAttention.Config
      self_attn:
        _target_: pkm.models.cloud.point_mae.PointMAESelfAttention.Config
        hidden_size: 256
        num_attention_heads: 4
        qkv_bias: true
        attention_probs_dropout_prob: 0.0
      output:
        _target_: pkm.models.cloud.point_mae.PointMAESelfOutput.Config
        hidden_size: 256
        hidden_dropout_prob: 0.0
    intermediate:
      _target_: pkm.models.cloud.point_mae.PointMAEIntermediate.Config
      hidden_size: 256
      intermediate_size: 128
      hidden_act: gelu
    output:
      _target_: pkm.models.cloud.point_mae.PointMAEOutput.Config
      intermediate_size: 128
      hidden_size: 256
      hidden_dropout_prob: 0.0
    hidden_size: 256
    layer_norm_eps: 1.0e-06
  num_hidden_layers: 4
